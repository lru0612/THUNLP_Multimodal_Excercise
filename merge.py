import torch
from peft import PeftModel
from mllm.model import MLLMModel
from transformers import AutoTokenizer
import os

# --- 1. å®šä¹‰æ¨¡å‹è·¯å¾„ ---
# åŸºç¡€æ¨¡å‹è·¯å¾„ (LoRAå¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹)
base_model_path = "/home/user-C/THUNLP_Multimodal_Excercise/output/grounding/fintuned_model_lr3e-5_batch1280_gradnorm1_use_lora_grounding100/merged_model"
# LoRA é€‚é…å™¨è·¯å¾„ (LoRAå¾®è°ƒåç”Ÿæˆçš„ checkpoint)
lora_adapter_path = "/home/user-C/THUNLP_Multimodal_Excercise/output/grounding/fintuned_model_lr1e-5_batch1280_gradnorm1_use_lora_grounding100_continued"
# åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
output_path = "/home/user-C/THUNLP_Multimodal_Excercise/output/grounding/fintuned_model_lr1e-5_batch1280_gradnorm1_use_lora_grounding100_continued/merged_model"

print(f"å³å°†åˆå¹¶æ¨¡å‹ï¼Œè¯·ç¡®è®¤è·¯å¾„ï¼š")
print(f"  - åŸºç¡€æ¨¡å‹: {base_model_path}")
print(f"  - LoRAé€‚é…å™¨: {lora_adapter_path}")
print(f"  - è¾“å‡ºç›®å½•: {output_path}")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(output_path, exist_ok=True)

# --- 2. åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ ---
print("\næ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨...")

# ä½¿ç”¨ device_map={"": 0} å°†æ•´ä¸ªæ¨¡å‹ç›´æ¥åŠ è½½åˆ° GPU 0
# è¿™é¿å…äº† 'auto' æ¨¡å¼ä¸‹çš„æ¨¡å—åˆ†å‰²é—®é¢˜ï¼ŒåŒæ—¶åˆ©ç”¨äº† GPU
model = MLLMModel.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map={"": 0}
)
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
print("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (å·²åŠ è½½åˆ° GPU)ã€‚")

# --- 2.5 æ·»åŠ ç‰¹æ®Šè¯ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰---
print("\næ­£åœ¨æ·»åŠ ç‰¹æ®Šè¯...")
new_special_tokens = [f"<Loc{i}>" for i in range(101)]
tokenizer.add_special_tokens(
    {"additional_special_tokens": new_special_tokens}
)
model.llm.resize_token_embeddings(len(tokenizer))
print(f"å·²æ·»åŠ  101 ä¸ªç‰¹æ®Šè¯ï¼Œè¯æ±‡è¡¨å¤§å°ç°åœ¨ä¸º: {len(tokenizer)}")

# --- 3. åŠ è½½å¹¶èåˆ LoRA é€‚é…å™¨ ---
print("\næ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨...")
# PeftModel ä¼šè‡ªåŠ¨å°†é€‚é…å™¨åŠ è½½åˆ°åŸºç¡€æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Š
model = PeftModel.from_pretrained(model, lora_adapter_path)
print("LoRA é€‚é…å™¨åŠ è½½å®Œæˆã€‚")

print("\næ­£åœ¨åˆå¹¶ LoRA æƒé‡...")
model = model.merge_and_unload()
print("æƒé‡åˆå¹¶å®Œæˆã€‚")

# --- 4. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ ---
print(f"\næ­£åœ¨å°†åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åˆ°: {output_path}")
# ä¿å­˜æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä» GPU ç§»å› CPUï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ
model.save_pretrained(output_path)
tokenizer.save_pretrained(output_path)
print("\nğŸ‰ åˆå¹¶åçš„æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸä¿å­˜ï¼")
