import torch
from peft import PeftModel
from mllm.model import MLLMModel
from transformers import AutoTokenizer
import os

# --- 1. å®šä¹‰æ¨¡å‹è·¯å¾„ ---
# åŸºç¡€æ¨¡å‹è·¯å¾„ (LoRAå¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹)
base_model_path = "/home/user-C/THUNLP_Multimodal_Excercise/MLLM_Excercise_Model"
# LoRA é€‚é…å™¨è·¯å¾„ (LoRAå¾®è°ƒåç”Ÿæˆçš„ checkpoint)
lora_adapter_path = "/home/user-C/THUNLP_Multimodal_Excercise/output/finetune/mllm_sft_training_batch80_lr4e-5_steps200_uselora_cosine_warmup30"
# åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
output_path = "/home/user-C/THUNLP_Multimodal_Excercise/MLLM_sft_training"

print(f"å³å°†åˆå¹¶æ¨¡å‹ï¼Œè¯·ç¡®è®¤è·¯å¾„ï¼š")
print(f"  - åŸºç¡€æ¨¡å‹: {base_model_path}")
print(f"  - LoRAé€‚é…å™¨: {lora_adapter_path}")
print(f"  - è¾“å‡ºç›®å½•: {output_path}")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(output_path, exist_ok=True)

# --- 2. åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ ---
print("\næ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨...")

# ä½¿ç”¨ device_map={"": 0} å°†æ•´ä¸ªæ¨¡å‹ç›´æ¥åŠ è½½åˆ° GPU 0
# è¿™é¿å…äº† 'auto' æ¨¡å¼ä¸‹çš„æ¨¡å—åˆ†å‰²é—®é¢˜ï¼ŒåŒæ—¶åˆ©ç”¨äº† GPU
model = MLLMModel.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map={"": 0}
)
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
print("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (å·²åŠ è½½åˆ° GPU)ã€‚")

# --- 3. åŠ è½½å¹¶èåˆ LoRA é€‚é…å™¨ ---
print("\næ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨...")
# PeftModel ä¼šè‡ªåŠ¨å°†é€‚é…å™¨åŠ è½½åˆ°åŸºç¡€æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Š
model = PeftModel.from_pretrained(model, lora_adapter_path)
print("LoRA é€‚é…å™¨åŠ è½½å®Œæˆã€‚")

print("\næ­£åœ¨åˆå¹¶ LoRA æƒé‡...")
model = model.merge_and_unload()
print("æƒé‡åˆå¹¶å®Œæˆã€‚")

# --- 4. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ ---
print(f"\næ­£åœ¨å°†åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åˆ°: {output_path}")
# ä¿å­˜æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä» GPU ç§»å› CPUï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ
model.save_pretrained(output_path)
tokenizer.save_pretrained(output_path)
print("\nğŸ‰ åˆå¹¶åçš„æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸä¿å­˜ï¼")
